# TwitterDisaster
Natural Language Processing with Disaster Tweets Predict which Tweets are about real disasters and which ones are not

### first started learning about NLP. It is basically the things I learned documented in Kaggle Notebook format. It can be helpful for you if you are looking for data analysis on competition data, feature engineering ideas for NLP, cleaning and text processing ideas, baseline BERT model or test set with labels.

### Meta Features:
### Now let us create some meta features and then look at how they are distributed between the classes. The ones that we will create are
1. Number of words in the text
2. Number of unique words in the text
3. Number of characters in the text
4. Number of stopwords
5. Number of punctuations
6. Number of upper case words
7. Number of title case words
8. Average length of the words

## Reference

### Visit https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc
### Visit https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings
### Visit https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing
### Visit https://www.kaggle.com/szelee/a-real-disaster-leaked-label
### Visit https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub

## Evaluation :- 

1. True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!
2. False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.
3. False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.



